ğŸ“„ Paper 1: â€œCross-Modality Image Synthesis from Unpaired Data Using Cycle-consistent Adversarial Networksâ€
Authors: Yi Zhou, Lei Bai, Debdoot Sheet, et al.
Source: IEEE Transactions on Medical Imaging, 2020
DOI: 10.1109/TMI.2020.2973022

âœ… Project Overview
This paper explores a deep learning framework for cross-modality medical image synthesis, particularly CT-to-MRI and vice versa, using unpaired training data. Traditional supervised models rely heavily on paired CT/MRI datasets, which are scarce due to ethical, logistical, and technical constraints. This work addresses that challenge using a CycleGAN-based architecture with medical-specific enhancements, enabling realistic image translation and structure preservation.

âš™ï¸ Technical Approach
The proposed model adapts CycleGAN for medical image domains with the following extensions:

Gradient Consistency Loss (GCL) â€“ added to preserve fine edge-level structures such as organ/tumor boundaries.

Identity Loss â€“ ensures the model behaves as an identity mapping when the source and target modality are the same.

Multi-scale training â€“ facilitates learning both global anatomical shapes and local textures.

These modifications make the system more robust for clinical applications, especially where synthetic images are used for diagnostics or treatment planning.

ğŸ§ª Data Preprocessing (with Expressions)
The data preprocessing pipeline is essential to standardize input from different modalities. It involves the following steps:

1. Intensity Normalization
Each input image 
ğ¼
I is normalized using z-score normalization:

ğ¼
norm
=
ğ¼
âˆ’
ğœ‡
ğ¼
ğœ
ğ¼
I 
norm
â€‹
 = 
Ïƒ 
I
â€‹
 
Iâˆ’Î¼ 
I
â€‹
 
â€‹
 
Where:

ğœ‡
ğ¼
Î¼ 
I
â€‹
 : Mean pixel intensity of the image

ğœ
ğ¼
Ïƒ 
I
â€‹
 : Standard deviation of pixel intensities

This brings MRI and CT intensities to a comparable range despite their inherent differences in Hounsfield units (CT) vs relative tissue contrast (MRI).

2. Histogram Matching
To reduce modality-related contrast shift:

ğ¼
matched
=
HistMatch
(
ğ¼
source
,
ğ¼
reference
)
I 
matched
â€‹
 =HistMatch(I 
source
â€‹
 ,I 
reference
â€‹
 )
Where HistMatch maps the CDF (cumulative distribution function) of the source to that of the reference modality (usually MRI â†’ CT).

3. Cropping and Resizing
Images are centrally cropped to a fixed patch size (e.g., 
256
Ã—
256
256Ã—256) to reduce irrelevant background and focus learning on anatomical regions.

4. Skull Stripping (Optional)
For brain MRIs, a binary mask is applied:

ğ¼
stripped
=
ğ¼
â‹…
ğ‘€
I 
stripped
â€‹
 =Iâ‹…M
Where 
ğ‘€
M is a binary brain mask obtained using tools like BET (Brain Extraction Tool).

ğŸ§  Model Architecture
Generators
Based on 9-block ResNet for image-to-image mapping.

Uses instance normalization and ReLU activations.

Outputs translated image 
ğº
(
ğ‘‹
)
âˆˆ
ğ‘Œ
G(X)âˆˆY.

Discriminators
PatchGAN classifiers (classify 
ğ‘
Ã—
ğ‘
NÃ—N patches as real/fake instead of full image).

Enables high-frequency texture learning.

Loss Functions
Adversarial Loss:

ğ¿
ğº
ğ´
ğ‘
(
ğº
,
ğ·
)
=
ğ¸
ğ‘¦
âˆ¼
ğ‘
data
(
ğ‘¦
)
[
log
â¡
ğ·
(
ğ‘¦
)
]
+
ğ¸
ğ‘¥
âˆ¼
ğ‘
data
(
ğ‘¥
)
[
log
â¡
(
1
âˆ’
ğ·
(
ğº
(
ğ‘¥
)
)
)
]
L 
GAN
â€‹
 (G,D)=E 
yâˆ¼p 
data
â€‹
 (y)
â€‹
 [logD(y)]+E 
xâˆ¼p 
data
â€‹
 (x)
â€‹
 [log(1âˆ’D(G(x)))]
Cycle Consistency Loss:

ğ¿
ğ‘
ğ‘¦
ğ‘
(
ğº
,
ğ¹
)
=
ğ¸
ğ‘¥
[
âˆ¥
ğ¹
(
ğº
(
ğ‘¥
)
)
âˆ’
ğ‘¥
âˆ¥
1
]
+
ğ¸
ğ‘¦
[
âˆ¥
ğº
(
ğ¹
(
ğ‘¦
)
)
âˆ’
ğ‘¦
âˆ¥
1
]
L 
cyc
â€‹
 (G,F)=E 
x
â€‹
 [âˆ¥F(G(x))âˆ’xâˆ¥ 
1
â€‹
 ]+E 
y
â€‹
 [âˆ¥G(F(y))âˆ’yâˆ¥ 
1
â€‹
 ]
Identity Loss (when input already belongs to target domain):

ğ¿
ğ‘–
ğ‘‘
(
ğº
)
=
ğ¸
ğ‘¦
[
âˆ¥
ğº
(
ğ‘¦
)
âˆ’
ğ‘¦
âˆ¥
1
]
L 
id
â€‹
 (G)=E 
y
â€‹
 [âˆ¥G(y)âˆ’yâˆ¥ 
1
â€‹
 ]
Gradient Consistency Loss:

ğ¿
ğ‘”
ğ‘Ÿ
ğ‘
ğ‘‘
(
ğº
)
=
âˆ¥
âˆ‡
ğº
(
ğ‘¥
)
âˆ’
âˆ‡
ğ‘¦
âˆ¥
1
L 
grad
â€‹
 (G)=âˆ¥âˆ‡G(x)âˆ’âˆ‡yâˆ¥ 
1
â€‹
 
Where 
âˆ‡
âˆ‡ denotes the Sobel gradient operator.

ğŸ“ˆ Evaluation Metrics
Used to measure the similarity between synthesized and target images:

SSIM (Structural Similarity Index): Measures structural similarity

PSNR (Peak Signal-to-Noise Ratio): Higher indicates better quality

MAE (Mean Absolute Error): Pixel-wise absolute error

Edge Error Rate: Evaluated using gradient maps

Expert qualitative assessment was also used to assess the preservation of anatomical structures.

ğŸ§© Challenges Addressed
Lack of aligned paired datasets for supervised training

Preserving organ/tumor boundaries and fine textures in generated images

Overcoming domain gaps in intensity distributions between CT and MRI

Ensuring realism while avoiding hallucinations or structure distortion

ğŸ¯ Expected Deliverables
Trained model that can synthesize CT from MRI (and vice versa) without paired data

Implementation of gradient-aware loss for structure preservation

Evaluation framework including both quantitative and expert-based metrics

Visualization tools for anatomical overlays of real vs synthetic images

Ablation study showing effect of each loss function

ğŸš€ Applications and Impact
Used in radiotherapy planning, where MRI â†’ CT conversion reduces radiation dose

Can augment multimodal datasets when certain modalities are missing

Helps in MRI-only workflows, making treatment planning safer and cheaper

Serves as a pretraining step for segmentation and registration tasks
